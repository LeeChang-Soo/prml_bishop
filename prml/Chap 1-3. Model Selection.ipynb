{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 모델 셀렉션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 다항식의 커브피팅 예제에서 least square 방식을 사용하였고, 다항식의 차수를 조절하여 일반화 할 수 있음을 보았다.\n",
    "  * 다항식의 자수가 모델의 파라미터 갯수를 조절하고 그렇게 하므로써, 모델의 복잡도를 조정한다. \n",
    "  * reguralized least square 에서, 정규화 계수 $\\lambda$ 도 모델의 복잡성을 효과적으로 관리한다. 그러나 더욱더 복잡한 모델에서는, 예를 들면 혼합 분포 모델(mixture distirbutor)나 신경망등에는 복잡도를 조정하는데 더욱더 많은 파라미터들이 있어야만 할 것이다.\n",
    "  * 실제 문제에서는, 조정하는데 주요한 항목들이 새로운 데이터를 예측하는데 좋은 성능을 나타나도록 할 것이다.\n",
    "  * 더구나, 주어진 모델에서 복잡한 파라미터들의 적절한 값을 찿아내야 할 뿐 아니라, 또한 여러 모델사이데 적절한 범위를 찿기를 원할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * MLE 접근방법에서 학습 데이터 셋트에 대한 성능이 over-fitting 문제 때문에 예측 성능에 좋은 것은 아니라는 것은 이미 알고 있다.\n",
    "  * 만일 데이터 셋트가 평범하다면, 한가지 접근 방법은 여러 모델에 가능한 데이터를 사용하던가 또는 복잡성 파라미터의 적절한 범위내의 값을 모델에 주던가 한 다음에, 소위 $validation\\,set$ 를 이용하여 각각 독립적인 데이터 셋트를 이용하여 모델을 비교한 후, 성능좋은 모델을 선택하면 된다.\n",
    "  * 만일 모델이 한정된 데이터 셋트를 여러번 반복해서 사용한다면, validation set 에서 over-fitting 이 발생하면, 선택된 모델을 최종평가 하기 위해서 test set 를 따로 보관 해야 할지도 모릅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 그러나 많은 시스템에서는 공급되는 학습데이터는 한정적일수 밖에 없으며 좋은 모델을 만들기 위해서는 학습용으 가능하면 많은 데이터를 제공해야만 할 것이다.\n",
    "  * 만일 validation set 가 너무 적다면, 예측 성능에 비교적 많은 노이즈가 있을 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이런한 문제를 해결하는 한가지 방법은, cross-validation(교차검증) 을 사용하는 것입니다.\n",
    "   \n",
    "   * 그림 1-18 cross-validation\n",
    " \n",
    " <img src=\"/files/books/prml/image/1-18cross-validation.png\" width=\"800\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 그림 설명 S-fold 교차검증 (S = 4 일 경우)\n",
    "   * 데이터를 S 그룹을 나누고\n",
    "   * S - 1 개 그룹을 학습용 데이타로, 나머지 하나를 검증용 데이타로 사용 ( 그림에서 붉은 색)\n",
    "   * S 번 반복\n",
    "   * 데이터가 부족할 경우 S = N 로 나눔 ( N 은 데이터 포인트) (이 경우 leave-one-out 기법이라고 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 교차검증의 한가지 단점은 수행해야 할 학습의 갯수가 S 배 만큼씩 증가하며, 컴퓨터 비용이 중요한 요소에서는 문제가 될 수 있음.\n",
    "   * 또다른 문제는 단일 모델인 경우 여러 복잡한 파라미터를 가질수 있다는 것이다.(예. 여러 정규화 파라미터 $\\lambda$ 등)\n",
    "   * 그러한 변수들의 조합은 최악의 경우 수행해야 할 학습및 평가 횟수가 급격하게 늘어 날 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 이상적으로는 테스트는 학습데이터에만 의존해야 하고, 여러 하이퍼 파라미터와 모델 선택이 한번의 학습에 의해서 결정 하는 것이다.\n",
    "  * 따라서, 학습데이터에만 의존하고, 오버피팅으로 인하여 bias 가 없는 성능측정 방법이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 복잡한 모델의 오퍼피팅을 보상하기 위한 페널티 항을 추가하면서 MLE 의 bias 를 수정하는 시도로 여러 'information criteria' 가 제안되어 왔다.\n",
    "  * 예를 들면, Akaike information criterion (AIC) 에서는 \n",
    "  * $\\ln p(\\mathcal D \\mid {\\bf w_{ML}}) - M$ 이 제이 큰값 을 같는 모델을 선택한다.\n",
    "      * $p(\\mathcal D \\mid {\\bf w_{ML}})$ : best-fit log likelihood\n",
    "      * M : 모델에서 조정 가능한 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Bayesian information criterion(BIC) 라고 불리는 AIC 의 변형이 4.4.1 절에서 논의 할 것이다.\n",
    "  * 그러나 그러한 기준들은 모델 파라미터에서 불확실성을 고려하지 않고, 지나치게 단순한 모델을 선호하는 경향이 있다.\n",
    "  * 따라서 3.4 절에서 완전한 베이지안 접근 방법을 검토하며, 복잡성 페널티가 자연스럽고, 원리적으로 발생하는것에 대해서 볼것이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

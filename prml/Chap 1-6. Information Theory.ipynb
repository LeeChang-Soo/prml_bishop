{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이장에서는 지금까지 확률이론과 결정이론에 관하여 논의 해 왔으며, 앞으로 계속된 논의에 기본이 된것이다.\n",
    "   * 이제 정보 이론에 관하여 마지막으로 논의 할 것이며, 기계학습과 패턴인식 개발에 아주 유용할 것이다.\n",
    "   * 여기서는 중요한 개념에 대해서 집중할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정보이득 (Information Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $x$ 를 이산확률변수라고 하자.\n",
    "   * $x$ 로 값을 받을 경우 얼마나 많은 정보를 받을 수 있을까???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 정보의 양은 $x$를 학습하는데, 놀라움 정도 ($degree\\,of\\,suprise$) 를 볼 수 있다. 만일 가능성이 없는 사건이 발생했다고 들었을때, 우리는 일상적으로 발생했을때 보다 더 많은 정보를 얻을 것이다. 그리고 만일 통상적인 어떤 사건이 발생한다는 것을 안다면, 아무런 정보를 얻을 수 없을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 정보 내용의 측정은 확률 $p(x)$ 에 관계 될것이며 $h(x)$ 으로 표시한다. $h(x)$ 는  확률 $p(x)$ 에 단조함수 (순서가 보존된) 이고, 정보내용을 나타낸다.\n",
    "   * 형식 $h(.)$ 은 서로 관계 없는 두 사건이 발생하면, 정보이득 Information Gain 은 그들 각각이 얻은 정보이득의 합으로 나타나냐 한다. 즉 $h(x, t) = h(x) + h(y)$.\n",
    "   * 두 관계없는 이벤트를 통계적으로는 독립사건이라고 하고 $p(x,y) = p(x)\\,p(y)$ 가 된다.\n",
    "   * 따라서 $h()$ 는 $p(x)$ 의 로그형태가 되며,  다음과 같은 형식이다.\n",
    " $$ h(x) = - \\log_2 p(x) \\qquad ({1.92})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 앞의 마이너스 (음수) 기호는 확률의 로그 값이 음수로 나와서 정보량은 양수로 만들기 위함이다.\n",
    "  * 로그의 기저는 임의적으로 선택할 수 있으나, 여기서는 가장많이 사용하는 2를 사용 할 것이다.\n",
    "  * 이 경우에는 $h(x)$ 의 단위가 'bits' 로 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 송신자는 확률변수 값을 수신자에게 보내려고 한다.\n",
    "   * 송신자가 보내는 평균 정보량은 확률분포 $p(x)$ 로 표현되며 ${\\bf 엔트로피}$ 라고 한다.\n",
    "\n",
    "$$H[x] = - \\sum_x p(x)\\log_2 p(x) \\qquad{(1.93)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이 양을 엔트로피라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\lim_{p \\to 0}$ 이므로 $p(x) = o$ 인 $x$ 를 만날때 마다, $p \\ln p = 0$ 를 취해야 할 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 엔트로피 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 지금까지는 휴리스틱한 방법으로 정보 정의 (1.92) 와 엔트로피 정의(1.93)를 하였다.\n",
    "   * 이러한 정의가 유용한 성질을 가지고 있음을 보이고자 한다.\n",
    "   * 확률변수 $x$ 가 8 가지 가능한 상태를 가지고 있고, 모든 상태가 동등한 가능성이 있다고 하자.\n",
    "   * $x$ 의 값을 전송하려면 3 bits 길이 메세지를 전송하여야 한다.\n",
    "   * 이값의 엔트로핀 는 $H[x] = -8 \\times \\dfrac{1}{8} \\,\\log_{2}\\dfrac{1}{8} = 3\\,bits$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비균일한 확률분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 8 개의 상태 $\\left \\{a, b, c, d, e, f, g, h \\right \\}$ 가 각 확률 상태가 $\\left \\{\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{16},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64} \\right \\}$ 로 주어진다.\n",
    "   * 이경우 엔트로피는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H[x] = - \\dfrac{1}{2}\\log_{2}\\dfrac{1}{2} - \\dfrac{1}{4}\\log_{2}\\dfrac{1}{4} - \\dfrac{1}{8}\\log_{2}\\dfrac{1}{8} - \\dfrac{1}{16}\\log_{2}\\dfrac{1}{16} - \\dfrac{1}{64}\\log_{2}\\dfrac{1}{64} - \\dfrac{1}{64}\\log_{2}\\dfrac{1}{64} - \\dfrac{1}{64}\\log_{2}\\dfrac{1}{64} - \\dfrac{1}{64}\\log_{2}\\dfrac{1}{64}  = 2\\,bits$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 확률분포가 균일한것 보다는 비균일한 경우 엔트로피는 더 적다.\n",
    "   * 그리고 무질서 항목으로 엔트로피를 해석할때 간단하게 어떤 통찰력을 얻을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비균일한 확률변수의 상태를 전송하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 균일한 3 bits 로 전송할 수 있으나, 이것을 많이 발생하는 경우 더 짧은 코드를 갖고, 덜 발생하는 이벤트에 대해서는 더 긴 코드를 갖는 그래서 평균적으로 더 짧은 코드 갖기를 희망하는 비균일한 분포를 고려해보자.\n",
    "   * 각 ${a, b, c, d, e, f, g, h}$ 상태에 다음 코드 스트링을 부여 하자. 0, 10, 110, 1110, 111100, 111101, 111102, 111110, 111111 .\n",
    "   * 평균 코드의 길이는 다음과 같이 된다\n",
    "$average\\,code\\,length = \\dfrac{1}{2} \\times 1 + \\dfrac{1}{4} \\times 2 + \\dfrac{1}{8} \\times 3 + \\dfrac{1}{16} \\times 4 + 4 \\times \\dfrac{1}{64} \\times 6 = 2\\,bits$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것은 위 경우와 같이 엔트로피와 같다.\n",
    "   * 짧은 코드인 경우에는 문자열을 재 사용 할 수 없는데, 그 이유는 두개 합치는 경우 예를들면 11001110 은 c, a, d 를 구분 할 수 없는 경우가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 엔트로피와 가장짧은 coding 길이와의 관계가 있으며, 샤논이 $noiseless\\,coding\\,theorem$ 에서 확률변수의 상태를 전송하는데 필요한 더 낮은 비트의 수가 엔트로피 라고 기술하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nats / bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 지금부터는 엔트로피 정의에 자연대수를 사용할 것이다. 자연대수를 사용하는 경우에는 \"bits\" 대신에  \"nats\" 단위를 사용하며, 단지 $\\ln 2$ 차이만 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 확률변수 상태 (통계역학) 로서 엔트로피 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 확률변수 상태를 표시하는 평균 정보의 양으로 엔트로피 개념을 소개 할 것이다.\n",
    "   * 엔트로피 개념은 물리에서 열평형의 개념으로 처음 소개 하였으며, 후에 통계역학에서 무질서를 측정하는 것으로 더 깊게 해석 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 동일한 개체 $n_i$ 가 $i^{th}$ 빈에 있는 전체 $N$ 개의 동등한 개체를 고려하는 시스템에서 엔트로피를 보면 무질서에 대한 개념을 이해 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 전체 N 개를 bin 에 넣는 경우를 생각하면\n",
    "      * 첫번째 개체는 $(N - 1)$ 번 선택을 할 수 있고\n",
    "      * 두번째 개체는 $(N - 2)$ 번 선택을 할 수 있고\n",
    "      * 마지막 N 개 까지 선택을 한다면.. \n",
    "      * 빈에 넣는 방법을 $N!$ 개의 경우가 있다.\n",
    "      * $i^{th}$ 빈에 있는 개체들은 서로 구분 할 수 없으므로 전체 경우수에 $n_i!$ 만큼을 나누어 주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 아주 간단한 예를 들어 보면 3 개의 구분을 할 수 없는 공이 있다고 하고 임시적으로 번호를 1, 2, 3 으로 부여 하자 {구분 할 수 없으므로 어디까지나 가정이다}\n",
    "   * 3 개의 공을 2개의 빈에 반드시 넣는 방법을 나열 해보면\n",
    "      * {1, 2} {3}\n",
    "      * {2, 1} {3}\n",
    "      * {1, 3} {2}\n",
    "      * {3, 1} {2}\n",
    "      * {2, 3} {1}\n",
    "      * {3, 2) {1}\n",
    "      * 모두 6가지가 나온다 $3! = 3 \\times 2 \\times 1 = 6$\n",
    "      * 이중 2개가 들어간 빈에서 공을 구분 할 수 없기 때문에 경우수 $2!$ 만큼 나누어 주어야 한다.\n",
    "      * 따라서 나열 할 수 있는 경우수는 3 가지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * N 개의 개체를 $n_i$ 개의 빈에 넣을수 있는 경우의 수는\n",
    "$$ W = \\dfrac{N!}{\\prod_i n_i!} \\qquad (1.94) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.94 multiplicity 형태이며, 계산하기 어렵다. 지금 구하고자 하는 것은 W 를 최대화 하는 경우수를 구하고자 하는 것인데 이 경우에도 곱하기 보다는 로그를 취하여 최대갑을 구하도록 한다. 로그는 단조 증가 함수이므로 로그에서 최대값을 나타내는 값은 원 함수에서도 최대값을 나타낸다.\n",
    "   * 엔트로피는 W 에 로그값을 취하여 $N$ 으로 나누어 준다.\n",
    "\n",
    "$$H = \\dfrac{1}{N}\\ln W=\\dfrac{1}{N}\\ln N!-\\dfrac{1}{N}\\sum_i \\ln n_i! \\qquad{(1.95)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stirling’s approximation (스터링 근사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식 $\\ln n! \\approx n\\ln x - n$  증명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\ln n! = \\sum_{k=1}^{n} \\ln k$\n",
    "   * n 이 충분히 크다면 합을 적분으로 계산 할 수 있다.\n",
    "   * $\\sum_{k=1}^{n} \\approx \\int_{1}^{n} \\ln x \\,dx$\n",
    "   * $ \\int_{1}^{n} x\\,dx$ 를 부분 적분하면 $[x\\ln x]_{1}^{n} - \\int_{1}^{n}x\\,\\dfrac{1}{x}\\, dx = [x\\ln x -x]_{1}^{n} = (n\\ln n - n) - (1 \\times 0 - 1) = n\\ln n - n - 1$\n",
    "   * $n \\gg 1$ 이므로 마지막 1을 무시 하면\n",
    "   * $\\ln n! \\approx  n \\ln n - n$ 가 얻는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다시 돌아가서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $N \\to \\infty$ 에서 $\\dfrac{n_i}{N}$ 이 어떤 값으로 수렴하고 그러면 스터링 근사를 취하여\n",
    "\n",
    "$$\\ln N! \\simeq N \\ln N - N \\qquad ({1.96}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H=-\\lim_{N \\to \\infty}\\sum_i\\dfrac{n_i}{N} \\ln (\\dfrac{n_i}{N}) = \\lim_{N\\to\\infty}\\sum_i p(x_i)\\ln p(x_i) \\qquad{(1.97)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 여기서 $\\sum_i n_i = N$ 과 $p_i = \\lim_{N \\to \\infty} \\frac{n_i}{N}$ 을 적용하였다.\n",
    "   * $p_i$ 는 개체ㅏ $i^{th}$ 에 들어갈 확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.95 을 적용하여 $N \\to \\infty$ 를 적용하고, $\\dfrac{n_i}{N}$ 이 어떤 값을 갖는다고 적용하면 식 1.97 이 나온다.\n",
    "   * $H = \\dfrac{1}{N}\\ln N! - \\dfrac{1}{N}\\sum_i \\ln n_i ! \\\\ = \\dfrac{1}{N}[N \\ln N - N] - \\dfrac{1}{N} \\sum_i [n_i \\ln n_i - n_i] \\\\ = \\ln N - \\dfrac{1}{N}\\sum_i n_i \\ln n_i = - [\\sum_i \\dfrac{n_i}{N}\\ln n_i - \\sum_i \\dfrac{n_i}{N}\\ln N] \\\\ = \\sum_i (\\dfrac{n_i}{N}) \\ln (\\dfrac{n_i}{N}) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### macrostate / microstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 물리적 용어로 빈에서의 배열을 microstate 라고 하고\n",
    "   * 빈을 차지하는 전반적인 배열을 macrostate 라고 하고 $\\dfrac{n_i}{N}$ 으로 표현된다.\n",
    "   * W 를 macrostate 의 weight 라고 알려져 있다.\n",
    "   * 좀 더 물리학적으로 말하면 다음과 같다.\n",
    "   * bin 은 이상기체에서 에너지를 어떤 에너지라고 표시한다. \n",
    "   * 그럼 각 이상기체는 어떤 특정 온도에서 각 이상기체 분자들은 각각의 에너지를 갖는다. 에너지를 빈이라고 하자.\n",
    "   * 어떤 특정 온도에서 어떤 에너지에를 갖는 분자들은 여러개가 된다. 그런 분자들은 여러 방향을 가질것이므로 각각의 운동량은 다를수 있다.\n",
    "   * 따라서 microstate 는 각 분자들이 갖는 운동량의 상태라고 포함하면 그런 분자들이 모여서 같은 에너지를 갖는다고 할 수 있다. 이것을 macrostate 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maximum 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 빈을 이산확률변수 $X$ 에서 $x_i$ 로 나타낼 수 있다. 즉 $p(X=x_i) = p_i$\n",
    "   \n",
    "$$H[p] = - \\sum_i p(x_i)\\ln p(x_i) \\qquad{(1.98)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 분포 $p(x_i)$ 는 어떤 특정값에 peak 일 경우에는 비교적 낮은 엔트로피를 갖지만, 분포가 넓게 퍼져 있으면 높은 엔트로피를 갖는다. 그림 1.30 에 잘 표시 되어있다.\n",
    "   \n",
    "   * 그림 1.30 엔트로피 히스토그램\n",
    "\n",
    "<img src=\"./image/1-30entropy.png\" width=\"800\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $p(x_i)$ 의 성질은 다음과 같다.\n",
    "      * $ 0 \\le p_i \\le 1$ 이고, 양수이므로 $p_i = 1$ 일때 최소값 0 이 된다. \n",
    "      * $p_{j\\ne i} = 0$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 최대 엔트로피는 H 를 최대화 할때 구할 수 있으므로 다음 조건식을 $\\sum_i p(x_i) = 1$ 구속식으로 하여 라그랑지 승수법을 이용한다.\n",
    " \n",
    " $$\\widetilde {H} = - \\sum_i p(x_i)\\ln p(x_i) + \\lambda\\left(\\sum_i p(x_i) - 1 \\right) \\qquad{(1.99)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $p(x_i)$ 가 모두 같은 값이고, $p(x_i) = \\dfrac{1}{M}$ (M 은 state $x_i$ 의 모든 상태이다) 일때 최대 값을 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것은 H 를 두번 미분하여 조사한다.\n",
    "\n",
    "$$\\dfrac{\\partial^2\\widetilde{H}}{\\partial p(x_i) \\partial p(x_j)}=-I_{ij}\\dfrac{1}{p_i} \\qquad{(1.100)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $I_{ij}$ 는 identity matrix 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속변수에 대한 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $x$ 를 연속변수로 확장하여 분포 $p(x)$ 를 연속함수로 보는 엔트로피를 정의 할 수 있다.\n",
    "   * 먼저 연속변소 $x$ 를 $\\Delta$ 넓이를 갖는 빈으로 나누자.\n",
    "   * 그럼 평균값 정리에 의해서\n",
    "$$\\int_{i\\Delta}^{(i+1)\\Delta}p(x)dx = p(x_i)\\Delta \\qquad{(1.101)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 적분 평균값 정리\n",
    "   * 연속함수  $f:[a,b] \\to \\mathbb R$ 에 대하여, 다음을 만족시키는 $c \\in (a, b)$ 가 존재한다.\n",
    "   * $\\int_{a}^{b} f(x)\\,dx = f(c)(a - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 변수 $x$ 를 양자화 하고, $i$ 번쨰 빈에 들어올 $x$ 를 $x_i$ 로 하면 $x_i$ 에서 관찰되는 확률은 $p(x_i)\\Delta$ 가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이제 이산 분포에 대한 엔트로피는 다음과 같이 된다.\n",
    "\n",
    "$$H_{\\Delta} = - \\sum_i p(x_i)\\Delta \\ln (p(x_i)\\Delta) = - \\sum_i p(x_i)\\Delta\\ln p(x_i) - \\ln \\Delta \\qquad{(1.102)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 여기서 $\\sum_i p(x_i)\\Delta = 1$ 을 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 식 1.102 에서 두번째 항 $- \\ln \\Delta$ 을 무시하고 첫번째 항에서 $\\Delta \\to 0$ 이면 1.102 는\n",
    "  \n",
    "$$\\lim_{\\Delta\\to 0}\\left\\{-\\sum_i p(x_i)\\Delta\\ln p(x_i)\\right\\} = - \\int p(x)\\ln p(x) dx \\qquad{(1.103)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것을 diffenential entropy 라고 한다. 이것은 단지 식 1.98 에서\n",
    "   * $H[p] = - \\sum_i p(x_i)\\ln p(x_i) \\qquad{(1.98)}$ 에서 $\\sum$ 을 $\\int$ 바꾸기만 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 그러나 연속일 경우와 이산적인 경우 엔트로피의 차이는 $\\ln \\Delta$ 이고 이것은 $\\lim \\Delta \\to 0$ 일 경우 발산한다.\n",
    "   * 이것은 연속변수를 매우 정확하게 지정하기 위해서는 매우 많은 비트를 요구하는것을 반영한다.\n",
    "   * 연속과 이산의 차이가 단기 기호만 기계적으로 바꾸어서 처리를 하면 되나, 이산적인 것이 정확하게 연속엔트로피와 같지는 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 다항 연속변수인 경우에, diffenential entropy 는 \n",
    "$$H[{\\bf x}] = - \\int p({\\bf x})\\ln p({\\bf x}) d{\\bf x} \\qquad{(1.104)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속변수에서 최대 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이산변수에서 최대 엔트로피는 동등한 확률분포를 갖는 것이다.\n",
    "   * 연속 변수에서는 최대 엔트로피는 어떨까??\n",
    "   * 연속변수에서 최대 엔트로피를 구하기 위해서 필요한 구속조건 normalization constant 이외에도 1차 2차 모멘트 구속조건이 필요하다.\n",
    "   * 모멘트의 정의는 $\\sum x^n p(x_i)$ 또는 $\\int x^n p(x_i) dx$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 따라서 구속조건을 다음과 같이 준다.\n",
    "  \n",
    "$$\\int_{-\\infty}^{\\infty} p(x)\\,dx = 1 \\qquad{(1.105)}$$\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} xp(x)\\,dx = \\mu \\qquad{(1.106)}$$\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} (x-\\mu)^2p(x)\\,dx = \\sigma^2 \\qquad{(1.107)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 위 제약조건을 라그랑지안 승수에 넣고 $p(x)$ 에 관해서 최대값을 풀면\n",
    "\n",
    "$$-\\int_{-\\infty}^{\\infty} p(x)\\ln p(x) dx + \\lambda_1\\left(\\int_{-\\infty}^{\\infty} p(x)dx - 1\\right) + \\lambda_2\\left(\\int_{-\\infty}^{\\infty} xp(x)dx-\\mu\\right) + \\lambda_3\\left(\\int_{-\\infty}^{\\infty}(x-\\mu)^2p(x)dx-\\sigma^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 변분원리를 이용하여 $\\lambda$ 에 대해서 풀면\n",
    "\n",
    "$$p(x) = \\exp\\left\\{-1+\\lambda_1+\\lambda_2x+\\lambda_3(x-\\mu)^2\\right\\} \\qquad{(1.108)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것을 3개의 구속조건에 넣고 다시 $\\lambda$ 에 대해서 풀면\n",
    "\n",
    "$$p(x) = \\dfrac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left\\{-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\qquad{(1.109)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 즉 differential entroy 의 분포를 최대화 하는것은 가우스 분포 이다.\n",
    "   * 엔트로피를 최대화 할때 분포가 negative 되는 조건은 없다.\n",
    "   * 그러나, 결과 분포가 사실은 nonnegative 이기 때문에 그런 조건이 필요없다는 것은 추후에 알게 될것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가우시안 분포의 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 가우시안 분포로서 엔트로피를 구하면\n",
    "\n",
    "$$H[x] = \\dfrac{1}{2}\\left\\{1+\\ln(2\\pi\\sigma^2)\\right\\} \\qquad{(1.110)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 엔트로피는 $\\sigma^2$ 가 증가하면, 즉 분포가 넓어지면 엔트로피는 증가된다.\n",
    "   * 이 결과는 diffenential entropy 가 discrete entropy 와 다르고, 음수가 될 수 있음을 보여준다.\n",
    "   * 즉 $H(x) < 0 \\,in \\,(1.110)\\, for\\, \\sigma^2 < \\dfrac{1}{2\\pi e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 조건부 엔트로피 (Conditional Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 결합 확률 $p({\\bf x}, {\\bf y})$ 을 알고 있다고 생각하자. \n",
    "   * ${\\bf x}$ 를 이미 알고 있고, 대응되는 ${\\bf y}$에 필요한 추가정보는 $-\\ln p({\\bf y} \\mid {\\bf x})$ 로 주어진다.\n",
    "   * 그럼, ${\\bf y}$ 에 지정된 추가 정보는 \n",
    "   \n",
    " $$H[{\\bf y}\\mid {\\bf x}] = - \\iint p({\\bf y}, {\\bf x})\\ln p({\\bf y}\\mid {\\bf x}) d{\\bf y}d{\\bf x} \\qquad{(1.111)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것을 조건부 엔트로피 Conditional Entropy 라고 한다.\n",
    "   * 조건부 엔트로피는 다음 조건을 만족한다.\n",
    "   \n",
    "   $$H[{\\bf x}, {\\bf y}] = H[{\\bf y}\\mid{\\bf x}] + H[{\\bf x}] \\qquad{(1.112)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 연관엔트로피 와 상호정보 (Relative entropy and mutual information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이 절에선 지금까지, 정보이론과 엔트로피의 기본개념 들에 대해서 소개해 왔다. \n",
    "   * 이제 패턴인식과 관련한 아이디어를 소개 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쿨백-라이블러 발산 (Kullback-Leibler Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 알려지지 않은 확률분호 $p({\\bf x})$ 를 생각하자.\n",
    "   * 이것을 확률 분포 $q({\\bf x})$ 에 근사하여 모델 하고자 한다.\n",
    "   * ${\\bf x}$ 값을 수신자에게 보낼 목적으로 적절한 코딩 구조를 만들기 위하여 확률분포 ${\\bf q}$ 를 사용한다면, 진짜 확률분포 $p({\\bf x})$ 대신에 $({\\bf x})$ 를 사용함으로서 추가적인 평쥰 정보량(nats 단위로) 이 필요한다. 이 추가적인 정보량은 다음과 같이 표현된다.\n",
    "   \n",
    "$$KL(p\\parallel q)= -\\int p({\\bf x})\\ln q({\\bf x})d{\\bf x} - \\left(-\\int p({\\bf x})\\ln p({\\bf x})d{\\bf x}\\right)\\\\ = - \\int p({\\bf x})\\ln\\left\\{\\dfrac{q({\\bf x})}{p({\\bf x})}\\right\\}d{\\bf x} \\qquad{(1.113)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이것을 $p({\\bf x})$ 와 $q({\\bf x})$ 사이의 relative entropy 또는 Kullback-Leibler divergence 또는 KL Divergence 라고 한다.\n",
    "   * 교환성질이 성립하지 않는다 즉 $KL(p \\parallel q) \\neq KL(q \\parallel p)$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쿨백-라이블러 발산 $\\ge 0$ 조건"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이제 $KL(q\\parallel q) \\ge 0$ 이 $p({\\bf x}) = q({\\bf x})$ 와 동치임을 보일려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### convex 함수  (블록함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 그림 1-31 Convex (블록) 함수\n",
    "\n",
    "<img src=\"./image/1-31convex.png\" width=\"400\" height=\"300\" align=\"left\">\n",
    "   * 모든 현(코드, 푸른선)은 함수선(붉은선) 또는 그 위에 놓이게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 함수 $f(x)$ 가 다음 조건을 만족하면 convex 라고 한다.\n",
    "      * 구간 $x=[a,b]$ 사이에, $f(x) = \\lambda a + (1 - \\lambda)b$ 여기서 $0 \\le \\lambda \\le 1$ 이다\n",
    "      * $x$ 에 대응하는 코드위 값은 $\\lambda f(a) + (1 - \\lambda)$ 이다.\n",
    "      * 대응하는 함수의 값은 $f(\\lambda a + (1 - \\lambda)b) 이다.\n",
    "      * 이런 조건에서 Convexity 란 다음을 만족한다.\n",
    " $$f(\\lambda{a}+(1-\\lambda)b) \\le \\lambda{f(a)} + (1-\\lambda)f(b) \\qquad{(1.114)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 이것은 함수의 2차 미분이 모든 구간에서 양수 인것과 동치 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 볼록함수의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $x \\ln x \\,\\, for (x \\gt 0)$ 또는 $x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### strictly convex 및 기타 성질들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.114 에서 $\\lambda = 1$ 또는 $\\lambda = 0$ 에서만 equality 가 성립하면 strictly convex(순볼록) 함수 이다.\n",
    "   * convex 하고 반대적인 성질을 가지고 있으면 concave (오목함수) 라고 하고, strictly convex 의 반대성질은 strictly concave 라고 한다.\n",
    "   * 함수 $f(x)$ 가 convex 이면 $-f(x)$ 는 concave 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 옌센(젠센) 부등식 (Jensen's inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * convex 성질을 이용하여 식(1.114) 로 부터 수학적 귀납법으로 (proof by induction) 함수 $f(x)$ 가 convex 이면 다음을 만족한다.\n",
    "   \n",
    " $$f\\left(\\sum_{i=1}^{M}\\lambda_ix_i\\right) \\le \\sum_{i=1}^{M}\\lambda_i f(x_i) \\qquad{(1.115)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 여기서 모든 $\\left \\{x_i \\right \\}$ 에 대해서 $\\lambda_i \\ge 0$ 이고 $\\sum_i \\lambda_i = 1$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.115 를 옌센부등식 이라고 한다.\n",
    "   * $\\lambda_i$ $\\left\\{x_i\\right\\}$ 를 갖는 이산변수 $x$ 의 를 확률분포로 해석하면 다음과 같다.\n",
    "\n",
    "$$f(\\mathbb E[x]) \\le \\mathbb E[f(x)] \\qquad{(1.116)}$$\n",
    "\n",
    "   * 여기서 $\\mathbb E[\\cdot]$ 는 기대값을 표현한다.\n",
    "\n",
    "   * 연속변수에서도 옌센부등식은 적용되며 다음과 같이 쓴다.\n",
    "   \n",
    "$$f\\left(\\int {\\bf x}p({\\bf x})d{\\bf x}\\right) \\le \\int f({\\bf x})p({\\bf x})d{\\bf x} \\qquad{(1.117)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL 발산 식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 옌센부등식을 KL 발산식에 적용하면..\n",
    "   \n",
    " $$KL(p\\parallel q) = - \\int p({\\bf x})\\ln\\left\\{\\dfrac{q({\\bf x})}{p({\\bf x})}\\right\\}d{\\bf x} \\ge - \\ln \\int q({\\bf x})d{\\bf x} = 0 \\qquad{(1.118)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 여기서 $\\ln\\left\\{\\dfrac{q({\\bf x})}{p({\\bf x})}\\right\\}$ 은 convex 함수이고\n",
    "   * $\\int q({\\bf x})\\,d{\\bf x} = 1$ 이다. 따라서 식(1.118) 의 마지막 항은 0 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $- \\ln x$ 는 strictly convex 함수이므로, 같은 경우와 모든 ${\\bf x}$ 에 대해서 $q({\\bf x}) = p({\\bf x})$ 일때만 equal (같다) 는 것은 동치이다.\n",
    "   * 따라서 위의 식에서 $KL[p \\parallel q]=0$ 이 만족되기 위해서는 $p=q$ 인 경우밖에 없다.\n",
    "   * 따라서 정의에 따라 $KL[p \\parallel q]=0$ 인 경우에만 $p({\\bf x})=q({\\bf x})$ 가 성립한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 그러므로  KL 발산 부등식은 두 분포함수 $p({\\bf x})$ 와 $q({\\bf x})$ 가 다름 정도를 측정하는것으로 해석 할 수 있다.\n",
    "      * $KL[p \\parallel q]$ 는 두 함수 p, q 가 같으면 0 이고 그 이외의 경우는 양의 실수를 같는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * data compression 과 확률분포추정 사이에 상세한 관계를 알 수 있다.\n",
    "      * 미확인 확률분포를 모델링 하는 문제\n",
    "      * 진짜 확률분포를 안다면, 가장 효율적인 압축을 얻을수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 만일 진짜 확률분포와 다른 확률분포를 사용하면, 반드시 덜 효율적인 코딩을 할 것이고, 추가 되는 정보의 평균을 보내야 하는데, 이것은 두 분포사이의 KL  발산과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL 발산의 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 모델로 하려는 미지의 확률분포 $p({\\bf x})$ 로 부터 생성되는 데이터 셋트를 생각하자.\n",
    "   * 이 분포를 어떤 모수분포 (parametric distribution) $q({\\bf q} \\mid \\pmb \\theta)$ 로 근사 할려고 한다.\n",
    "       * $\\theta$는 조정가능하고\n",
    "       * 대표적으로 생각할 수 있는것은 다변량 가우스분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\theta$를 결정하는 한가지 방법은 KL 발산을 최소화 하는 것이다.\n",
    "      * $p({\\bf x})$ : 진짜 분포\n",
    "      * $q({\\bf x} \\mid {\\pmb \\theta})$ : 근사하려는 분포\n",
    "      * KL Divergence 는 p 분포와 q 분포의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $p({\\bf x})$ 는 알 수 없으므로 직접 계산 할 수 없다.\n",
    "   * 그러나 $p({\\bf x})$ 로 부터 나온 한정된 데이터 셋트(학습데이터) 는 있다.\n",
    "       * $x_n$ , $n = 1,...,N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.35 기억하면 $\\mathbb E[f]\\simeq\\dfrac{1}{N}\\sum_{n=1}^{N}{f(x_n)} \\qquad{(1.35)}$\n",
    "   * $p({\\bf x})$ 는 학습데이터로 부터 근사가 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * KL Divergence 는\n",
    "$$KL(p\\parallel q) \\simeq \\frac{1}{N}\\sum_{n=1}^{N}\\left\\{-\\ln q({\\bf x}_n \\mid {\\pmb \\theta}) + \\ln p({\\bf x}_n)\\right\\} \\qquad{(1.119)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 와 같이 되는데, 이것은 $\\ln\\left\\{\\dfrac{q({\\bf x})}{p({\\bf x})}\\right \\}$ 을 풀어서 전개한것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.119 의 두번째 항은 ${\\bf \\theta}$ 에 무관한 항이고,\n",
    "   * 첫번째 항은 학습데이터로 부터 러닝된 확률분포 $q({\\bf x} \\mid {\\pmb \\theta})$ 의 ${\\pmb \\theta}$ 에 대한 negative log likelihood 함수 이다.\n",
    "   * 따라서, KL Divergence 를 최소화 하는것은 likelihood function 을 최대화 하는것과 동치이다.\n",
    "   * 변분 추론에 다시 한번.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상호정보 (Mutual Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 결합확률 $p({\\bf x}, {\\bf y})$ 에 의해 주어진 두 변수 ${\\bf x}$, ${\\bf y}$ 의 데이터 셋 사이의 결합확률을 생각하자.\n",
    "       * 두 변수 가 독립이면, 그들 각각의 주변화된 (marginal) 곱으로  $p({\\bf x}, {\\bf y}) = p({\\bf x}) p({\\bf y})$ 로 된다.\n",
    "       * 독집적이지 않으면, 결합확률과, 그들 각각 주변분포(marginal) 의 곱 과 차이로서 KL Divergence 를 고려하여 그들 각각이 독립적인것에 \"근접하는지\" 여부를 알 수 있는 어떤 아이디어를 얻을수 있다.\n",
    "       * 결합분포와 주변분포 각각의 곱과의 차이를 다음과 같이 정의 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I[{\\bf x}, {\\bf y}] \\equiv KL(p({\\bf x}, {\\bf y}) \\parallel p({\\bf x})p({\\bf y})) \\\\ = - \\iint p({\\bf x}, {\\bf y})\\ln\\left(\\dfrac{p({\\bf x})\\,p({\\bf y})}{p({\\bf x}, {\\bf y})}\\right)\\,d{\\bf x}d{\\bf y} \\qquad{(1.120)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 식 1.120 을 상호정보 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * KL 성질로 부터\n",
    "        * ${\\bf x}$ 와 ${\\bf y}$ 가 독립적이라는 것과, $I({\\bf x}, {\\bf y}) \\ge 0$ 이 equality 라는 것은 동치이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 확률의 합과 곱 법칙을 이용하면, mutual information 은 conditional entropy 와 관계있음을 알수 있다.\n",
    " \n",
    " $$I[{\\bf x}, {\\bf y}] = H[{\\bf x}] - H[{\\bf x}\\mid{\\bf y}] = H[{\\bf y}] - H[{\\bf y} \\mid {\\bf x}] \\qquad{(1.121)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 상호정보 (Muaual Information) 을 알면, 주어진 ${\\bf y}$를 알면, ${\\bf x}$ 의 불확실성을 줄이던지 또는 그 반대인 경우로 알 수 있다.\n",
    "   * 베이지안 관점에서, p({\\bf x}) 를 ${\\bf x} 에 대한 사전분포, $p({\\bf x} \\mid {\\bf y})$ 를 신규 관측된 ${\\bf y} 에 대한 사후 분포이다.\n",
    "   * 그러므로, mutual information 은 신규 관측치 ${\\bf y}$ 에 대한 결과로서 ${\\bf x}$ 에 대한 불확실성을 줄이는 것을 나타낸다.\n",
    "   *  이부분은 좀더 study 후에 깊게 이해가 될 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

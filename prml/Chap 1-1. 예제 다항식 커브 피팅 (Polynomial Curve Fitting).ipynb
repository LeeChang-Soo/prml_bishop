{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simple Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이 예제를 이용하여 이번장에 필요한 개념을 설명\n",
    "   * 관측된 변수 $x$ 와 관측된 데이터를 이용하여 $t$ 를 예측하려고 함.\n",
    "   * 관측된 데이터는 $\\sin(2\\pi x)$ 즉 타겟 데이터로 생성된 것에 난수(노이즈) 를 더하여 생성한다. \n",
    "   * 관측값 $x$ 에서 $N$ 개의 학습데이터가 있다.\n",
    "   * 이것을 $\\textbf{x} \\equiv (x_1,...,x_N)^T$ 로 표시하고, 결과 Target 을 $\\textbf{t} \\equiv (t_1,...,t_N)^T$ 로 표시 한다.\n",
    "   * 다음의 그림은 $N = 10$ data points 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1.2 $N = 10$ Points 에서 학습데이터 그래프 이다.\n",
    "\n",
    "<img src=\"/files/books/prml/image/sinx.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 푸른 원은 입력 $x$ 에 대해서 각 타겟 관측을 나타낸다.\n",
    "   * 녹색선은 데이터를 생성하는 $\\sin(2\\pi x)$ 이다.\n",
    "   * 새로운 $\\textbf{x}$ 에 대해서 $\\textbf{t}$ 값을 녹색선에 대한 지식이 없이 예측하는 것이다.\n",
    "   * $x_n$ , $n = 1, ..., N$ 은 구간 $[0,1]$ 에서 균등하게 배열 함.\n",
    "   * $\\textbf{t}$ 는 $\\textbf{x}$ 의 각 $x_n$ 에 대응하는 $sin(2\\pi x)$ 에 정규분포(Gausian distribution)의 랜덤 값을 더한 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 목표는 주어진 학습 데이터로 부터 학습을 하여 어떤 신규 $x$ 값 $\\hat{x}$ 에 대해서 $\\hat{t}$ 를 구하고자 하는 것이다.\n",
    "  * 그렇지만 여기에는 주어진 한정 데이터로 부터 일반화 하여야 하는 어려운 문제점이 있다.\n",
    "  * 여기에 추가하여 관측되는 데이터는 어느정도 노이즈가 포함 되어 있으므로 주어진 $\\hat{x}$ 에 대해서 $\\hat{t}$ 는 어느정도 불확정한 것이 된다.\n",
    "  \n",
    "#### 확률이론은 정확하고 양적인 방법으로 이러한 불확정한 정도를 나타나는 방법에 대해서 논의 할 것이며\n",
    "##### 결정이론(Decision Thoery) 는 적절하고 최적화된 예측을 하기 위하여, 확률적 표현을 사용 하도록 할 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 커브 피팅에 대한 단순하고 일반적인 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 커브피팅을 하기 위하여 간단하게 다항식 형태를 사용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y(x,{\\bf w})=w_0+w_1x+w_2x^2+...+w_Mx^M=\\sum_{j=0}^{M}w_jx^{;j} = {\\bf w}^T{\\bf x}\\qquad{(1.1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 다항식 계수 $w_0,...,w_M$ 는 벡터 $\\bf w$ 로 표시 한다.\n",
    "  * $y(x, \\bf w)$ 는 계수 $\\bf w$ 에 대하여 선형 함수 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이러한 모델에 대한 풀이는 3장 4장에서 상세히 다룰것이다.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 주어진 테스트 데이터에 의하여 다항식의 계수는 결정할 것이다. 일반적으로 가우스 제곱방식으로 정한다.\n",
    "  * 학습 데이터 와 $y(x, \\bf w)$ 의 차이를 $error \\, function$ 라 하고, 이 함수의 값이 최소화 하도록 $\\bf w$ 를 정한다.\n",
    "  * 가장 단순하면서 일반적인 방법은 $x_n$ 에서 $y(x, \\bf w)$ 과 실제 학습데이터 $t_n$ 과의 차이의 제곱의 합으로 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E({\\bf w})=\\dfrac{1}{2}\\sum_{n=1}^{N}\\left\\{y(x_n,{\\bf w})-t_n \\right\\}^2 \\qquad{(1.2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\dfrac{1}{2}$ 는 편의상으로 도입한것이며, $E(\\bf w)$ 를 최소화 하는 $\\bf w$ 를 정하는 것이다.\n",
    "   * 일단 모든 $y(x, \\bf w)$ 가 모든 모든 학습데이터를 지나가면 $E(\\bf w)$ 는 0 이 될 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $E(\\bf w)$ 의 기하학적 의미는 다음과 같다. 그림 1.3 과 같다. 즉 일반적으로 곡선의 접선에 수직인 방향이 아닌 $y$ 축에 평행인 방향으로 차이.\n",
    "   * 그 이유는 계산이 편해서임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 그림 1-3 $E(\\bf w)$ 의 기하학적 의미\n",
    "\n",
    "<img src=\"/files/books/prml/image/error_meaning.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $E(\\bf w)$ 가 최소 되도록 $\\bf w$를 설정한다. \n",
    "   * $E(\\bf w)$ 가 $\\bf w$ 에 2차 함수 이므로 최소가 되는 미분은 1차 함수가 되어 최소가 되는 해는 유일하게 결정된다.\n",
    "   * 이때 $\\bf w$ 를 $\\bf w^*$ 로 표시하면 최소가 되는 함수는 $y(x, \\bf w^*)$ 가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 커브 피팅의 또다른 결정 문제는 Order M 을 결정 하는 문제이다.\n",
    "   * $model\\,comparison\\, or\\, model\\, selection$ 라고 한다.\n",
    "   * 간단하게 그림 1.4 에 $M = 0,1,3,$ and $9$ 로 나누어서 표시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 그림 1.4 다양한 형태의 M 에 따른 그래프\n",
    "\n",
    "<img src=\"/files/books/prml/image/model-select.png\" width=\"800\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * $(M = 0), (M = 1)$ 인 경우에는 주어진 학습데이터에 잘 맞지 않는다. \n",
    "  * $(M = 2)$ 인 경우 비교적 잘 맞는 것 같음.\n",
    "  * $(m = 9)$ 인 경우 모든 학습자료하고는 잘 맞으나, 즉 $E(\\bf w^*) = 0$ 이다. 그러나, 너무 심하게 교란하고 있으므로 좋은 해결은 아니다.\n",
    "  * 이런 경우 Over-fitting 이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델에 대한 평가 (M 선택에 따른 비교)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 신규 데이타에 대비해서 가장 잘 예측 할 수 있도록 일반화 하는 것이 목적이다.\n",
    "  * 이것은 각 $M$ 에 대해서 성능을 비교하여 가장 좋은 결과를 얻도록 하는 것이다.\n",
    "  * 즉 각 학습 데이터에 대해서 $E(\\bf w^*)$ 의 잔차(Residual)(샘플링 오차) 를 평가 하도록 한다.\n",
    "  * 잔차를 평가하는 방법은 roor-mean-square 를 이용한다. 이것은 다음과 같이 정의 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{RMS}=\\sqrt{\\frac{2E({\\bf w}^*)}{N}} \\qquad{(1.3)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 그림 1.5 M 에 따른 root-mean-square 비교 \n",
    "\n",
    "<img src=\"/files/books/prml/image/rms.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * M = 3 인 경우 그림 1.4 에서 가장 잘 맞는 경우임. $ 0 \\leq M \\leq 2$ 인 경우 rms 는 비교적 크다.\n",
    "   * $ 3 \\leq M \\leq 8$ rms 가 적절한 값이다.\n",
    "   * $M = 9$ 인 경우 학습 데이터는 rms 가 0 이나 테스트 데이터는 매우 크다. 즉 그림 1.4 에서 테스트 데이터는 매우 크다.\n",
    "   * 그렇지만 그림 1-4에서 보면 $M=9$ 인 경우에는 매우 요동이 크다.\n",
    "   * $M=9$ 인 경우에는 낮은 차수 항목들을 다 포함 하고 있기 때문에 $M=3$ 에서와 같은 결과도 같이 포함 하고 있을 수 있다.\n",
    "   * 이것은 $\\sin(2 \\pi x)$ 를 테일러 급수 전개 해보면 알 수 있는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지수 (Order M) 의 크기에 대한 조사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 표 1-1 : 다항식에서 $w^*$ 의 지수 M 의 크기에 따른 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/files/books/prml/image/wvalue.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 표 1-1 에서 $M$ 이 증가 함에 따라 일반적으로 계수도 증가함.\n",
    "   * $M=9$ 일 경우 $w^*$ 는 잘 데이터에 잘 tune 된다. $w^*$ 는 교대라 큰 수로 음수, 양수 교대로 변화한다.\n",
    "   * 이것은 그림 1-4 에서 $M=9$ 보면 요동으로 나타난다.\n",
    "   * 직관적으로 보면 $M$ 이 증가함에 따라, 타겟 값에서 노이즈(랜덤값)은 잘 tune 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타 크기에 대한 조사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1-6 데이타 사이즈, $M=9$ 에서 $N=15$, $N=100$ 일 경우. 데이터의 크기가 증가하면 Over-fitting 문제가 해결 될 수 있음을 볼수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/files/books/prml/image/data-size.png\" width=\"500\" height=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 데이터가 많으면 많을수록 모델은 더 유연하게 tune 할 수 있다.\n",
    "   * 대충 휴리스틱하게 보면 모델의 적절한 파라미터의 5 - 10(w 가 9 이면 데이터 크기는  45 - 100) 배 보다는 작으면 안된다.\n",
    "   * 그러나 제 3 장에서 보면, 모델의 복잡성을 측정하는데, 파라미터의 숫자는 중요하지 않다.\n",
    "   * 학습데이터의 크기에 따라서 모델의 파라미터의 숫자에 제한을 둔다는 것이 만족하지 않을 수 있다.\n",
    "   * 그것 보다는 해결 할려고 하는 문제의 복잡성에 따른 모델의 복잡성을 선택하는 것이 더 합리적으로 보인다.\n",
    "   * 모델 파라미터를 찿기 위하여, least-square 방법을 선택하는데, 최대우도 (maximum likihood) 의 특별한 경우를 볼것이다(섹션 1,2.5)\n",
    "   * 그리고 over-fitting 문제는 maximum likihood 의 일반적인 성질로서 이해될 것이다.\n",
    "   * 베이시안 방법을 선택하므로써, Over-fitting 문제는 해결 할 수 있을 것이다.\n",
    "   * 데이타보다 많은 파라미터의 갯수를 이용하는 베이지안 관점에서는 문제가 없을 것이고, 효율적인 파라미터수는 데이터의 크기에 자연스럽게 포함된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화 기법 (Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 지금 접근하는 방법에서 유연하고 복잡한 모델을 사용하는데 제한된 데이터 크기에 적용할 실제적인 접근방법 소개\n",
    "   * Over-fitting 을 효과적으로 제어 할 수 있는 방법\n",
    "   * $error function$ 에 계수가 큰값을 갖지 못하도록 페널티항을 추가하는 기법\n",
    "   * 패널티 항은 간단한 방법으로 모든 계수의 제곱의 합의 항으로 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{E}({\\bf w})=\\dfrac{1}{2}\\sum_{n=1}^{N}\\left\\{y(x_n,{\\bf w})-t_n \\right\\}^2+\\dfrac{\\lambda}{2}|{\\bf w}|^2\\qquad{(1.4)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\|{\\bf w}\\|^2\\;{\\equiv}\\;{\\bf w}^T{\\bf w}\\;{\\equiv}\\;w_0^2+w_1^2+...+w_M^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\lambda$ 는 $sum$-$of$-$squares$ $error$ $term$ 에 대비하여 상대적인 중요성을 조정하도록 한다.\n",
    "   * $w_0$ 는 종종 제외한다.\n",
    "   * 식 (1.4) 도 최소값이 항상 존재한다.\n",
    "   * 통계학에서는 이러한 기법을 $shrinkage$ 방법이라고 하느데, 이유는 계수의 값을 작게 조정 하기 때문이다.\n",
    "   * 식 (1.4) 처럼 정규화 항 $\\lambda$ 항이 $\\bf w$ 의 2차항으로 되는 것을 $ridge$ $regression$ 이라고 한다.\n",
    "   * 신경망에서는 이러 기법을 $weight decay$ 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1-7 $M=9$ 에서 Regularization 항을 사용하여 $\\ln \\lambda = 18$ 과 $\\ln \\lambda = 0$ 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/files/books/prml/image/lambda.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\ln \\lambda = -18$ 인 경우 over-fitting 이 줄어 들며, 함수 $\\sin(2 \\pi x)$ 에 가까워 지고 있다.\n",
    "   * $\\lambda$ 를 너무 크게 잡으면 그림 1-7 에서 오른쪽, fit 가 잘 되지 않는다.\n",
    "   * 표 1-2 에 regularization 항이 어떻게 영향을 주는지 나타나 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표1-2 $M=9$ 에서 $\\bf w$ 의 $\\lambda$에 대한 영향. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/files/books/prml/image/lamda-effect.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * $\\ln \\lambda = \\infty$ - no regulation  $\\lambda$ 가 증가하면 계수 크기는 줄어들게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS error (1.3)  및 $\\ln \\lambda$ 의 학습및 테스트 데이터에 대한 영향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1-8 $M=9$ 에서 $root$-$mean$-$square$ $error$ 에 대한 $\\ln \\lambda$ 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/files/books/prml/image/rms-lambda-effect.png\" width=\"400\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 모델의 복잡성은 중요한 이슈이며, 1-3에서 논의 할 것이다.\n",
    "   * $error\\,function$ 을 최소화 하는 기법을 사용하려면, 모델에 적절한 복잡도를 적용해야만 한다. (모델의 파라미터 갯수??)\n",
    "   * 계수 $\\bf w$ 및 validation set (hold-out set)을 사용하여 모델의 복잡도 ($M$ , $\\lambda$) 를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 보통 training set (50%), validation set(25%), test set(25%) 로 분리하는함.\n",
    "      * training set 은 모델을 빌드 하는데 사용\n",
    "      * validation set 를 이용하여 prediction error 를 검증하고\n",
    "      * test set 은 일반화 과정상의 에러를 검출하는데 사용함.\n",
    "   * test set 인 경우 모델의 모든 tuning 과정이 끝날때 까지는 사용하지 말아야 함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
